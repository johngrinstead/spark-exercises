{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     n group\n",
       "0    0     b\n",
       "1    1     b\n",
       "2    2     c\n",
       "3    3     a\n",
       "4    4     c\n",
       "5    5     c\n",
       "6    6     a\n",
       "7    7     b\n",
       "8    8     a\n",
       "9    9     b\n",
       "10  10     b\n",
       "11  11     a\n",
       "12  12     b\n",
       "13  13     a\n",
       "14  14     b\n",
       "15  15     b\n",
       "16  16     c\n",
       "17  17     c\n",
       "18  18     a\n",
       "19  19     c"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create pandas dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(456)\n",
    "\n",
    "pandas_dataframe = pd.DataFrame(\n",
    "    dict(n=np.arange(20), group=np.random.choice(list(\"abc\"), 20))\n",
    ")\n",
    "pandas_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[n: bigint, group: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark can convert any pandas dataframe into a spark dataframe\n",
    "df = spark.createDataFrame(pandas_dataframe)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  n|group|\n",
      "+---+-----+\n",
      "|  0|    b|\n",
      "|  1|    b|\n",
      "|  2|    c|\n",
      "|  3|    a|\n",
      "|  4|    c|\n",
      "+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark is 'lazy', in that it will not display data until explicitly told to do so\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, n: string, group: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .describe will give you info on the column dtypes\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-----+\n",
      "|summary|                n|group|\n",
      "+-------+-----------------+-----+\n",
      "|  count|               20|   20|\n",
      "|   mean|              9.5| null|\n",
      "| stddev|5.916079783099616| null|\n",
      "|    min|                0|    a|\n",
      "|    max|               19|    c|\n",
      "+-------+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# once again you must summon the data with .show() if you wish to see it though\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|   a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|   a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|  auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|   a4|  2.8|1999|  6|  auto(l5)|  f| 16| 26|  p|compact|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydataset import data\n",
    "\n",
    "# make spark dataframe from mpg data\n",
    "mpg = spark.createDataFrame(data(\"mpg\"))\n",
    "mpg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'hwy'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is an invalid way to select a column, unlike pandas\n",
    "mpg.hwy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hwy: bigint, cty: bigint, model: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .select is needed to call on any specific column\n",
    "mpg.select(mpg.hwy, mpg.cty, mpg.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----------+\n",
      "|hwy|cty|     model|\n",
      "+---+---+----------+\n",
      "| 29| 18|        a4|\n",
      "| 29| 21|        a4|\n",
      "| 31| 20|        a4|\n",
      "| 30| 21|        a4|\n",
      "| 26| 16|        a4|\n",
      "| 26| 18|        a4|\n",
      "| 27| 18|        a4|\n",
      "| 26| 18|a4 quattro|\n",
      "| 25| 16|a4 quattro|\n",
      "| 28| 20|a4 quattro|\n",
      "+---+---+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data still won't appear until you use .show()\n",
    "mpg.select(mpg.hwy, mpg.cty, mpg.model).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'(hwy + 1)'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can do simple arithmetic\n",
    "mpg.hwy + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|hwy|(hwy + 1)|\n",
      "+---+---------+\n",
      "| 29|       30|\n",
      "| 29|       30|\n",
      "| 31|       32|\n",
      "| 30|       31|\n",
      "| 26|       27|\n",
      "+---+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this will return a new column with the the number 1 added to the number in the hwy column\n",
    "mpg.select(mpg.hwy, mpg.hwy + 1).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|highway_mileage|\n",
      "+---------------+\n",
      "|             29|\n",
      "|             29|\n",
      "|             31|\n",
      "|             30|\n",
      "|             26|\n",
      "+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .alias will allow you to rename a column in a spark dataframe\n",
    "mpg.select(mpg.hwy.alias(\"highway_mileage\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+\n",
      "|highway_mileage|highway_mileage_halved|\n",
      "+---------------+----------------------+\n",
      "|             29|                  14.5|\n",
      "|             29|                  14.5|\n",
      "|             31|                  15.5|\n",
      "|             30|                  15.0|\n",
      "|             26|                  13.0|\n",
      "+---------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# columns can also be stored as variables to be called on later\n",
    "col1 = mpg.hwy.alias(\"highway_mileage\")\n",
    "col2 = (mpg.hwy / 2).alias(\"highway_mileage_halved\")\n",
    "mpg.select(col1, col2).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other ways to create columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'hwy'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# col(column) can be used to summon a specific column as well\n",
    "col(\"hwy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-----------+\n",
      "|highway_mileage|city_mileage|avg_mileage|\n",
      "+---------------+------------+-----------+\n",
      "|             29|          18|       23.5|\n",
      "|             29|          21|       25.0|\n",
      "|             31|          20|       25.5|\n",
      "|             30|          21|       25.5|\n",
      "|             26|          16|       21.0|\n",
      "+---------------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make a new feature by perfrom algebra on 2 or more columns, save to variable\n",
    "avg_column = (col(\"hwy\") + col(\"cty\")) / 2\n",
    "\n",
    "# use variable as a feature in new dataframe\n",
    "mpg.select(\n",
    "    col(\"hwy\").alias(\"highway_mileage\"),\n",
    "    mpg.cty.alias(\"city_mileage\"),\n",
    "    avg_column.alias(\"avg_mileage\"),\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------------+-------------------+\n",
      "|hwy|(hwy + 1)|highway_mileage|highway_incremented|\n",
      "+---+---------+---------------+-------------------+\n",
      "| 29|       30|             29|                 30|\n",
      "| 29|       30|             29|                 30|\n",
      "| 31|       32|             31|                 32|\n",
      "| 30|       31|             30|                 31|\n",
      "| 26|       27|             26|                 27|\n",
      "+---+---------+---------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# expr(column) is essentially the same as col(), however it allows for manipulation of the data\n",
    "mpg.select(\n",
    "    expr(\"hwy\"),  # the same as `col`\n",
    "    expr(\"hwy + 1\"),  # an arithmetic expression\n",
    "    expr(\"hwy AS highway_mileage\"),  # using an alias\n",
    "    expr(\"hwy + 1 AS highway_incremented\"),  # a combination of the above\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+\n",
      "|highway|highway|highway|highway|\n",
      "+-------+-------+-------+-------+\n",
      "|     29|     29|     29|     29|\n",
      "|     29|     29|     29|     29|\n",
      "|     31|     31|     31|     31|\n",
      "|     30|     30|     30|     30|\n",
      "|     26|     26|     26|     26|\n",
      "+-------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# all methods are valid for returning the same data\n",
    "mpg.select(\n",
    "    mpg.hwy.alias(\"highway\"),\n",
    "    col(\"hwy\").alias(\"highway\"),\n",
    "    expr(\"hwy\").alias(\"highway\"),\n",
    "    expr(\"hwy AS highway\"),\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg.createOrReplaceTempView(\"mpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[hwy: bigint, cty: bigint, avg: double]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .sql('query') will allow you to use syntactically correct sql queries on the data in your df\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "SELECT hwy, cty, (hwy + cty) / 2 AS avg\n",
    "FROM mpg\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|hwy|cty| avg|\n",
      "+---+---+----+\n",
      "| 29| 18|23.5|\n",
      "| 29| 21|25.0|\n",
      "| 31| 20|25.5|\n",
      "| 30| 21|25.5|\n",
      "| 26| 16|21.0|\n",
      "+---+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we must still use the .show() command\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "SELECT hwy, cty, (hwy + cty) / 2 AS avg\n",
    "FROM mpg\n",
    "\"\"\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type Casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('manufacturer', 'string'),\n",
       " ('model', 'string'),\n",
       " ('displ', 'double'),\n",
       " ('year', 'bigint'),\n",
       " ('cyl', 'bigint'),\n",
       " ('trans', 'string'),\n",
       " ('drv', 'string'),\n",
       " ('cty', 'bigint'),\n",
       " ('hwy', 'bigint'),\n",
       " ('fl', 'string'),\n",
       " ('class', 'string')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show column dtypes\n",
    "mpg.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- displ: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- cyl: long (nullable = true)\n",
      " |-- trans: string (nullable = true)\n",
      " |-- drv: string (nullable = true)\n",
      " |-- cty: long (nullable = true)\n",
      " |-- hwy: long (nullable = true)\n",
      " |-- fl: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# will also display column dtypes\n",
    "mpg.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hwy: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .cast('string') will convert the dtype of the column to a string\n",
    "mpg.select(mpg.hwy.cast(\"string\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|model|model|\n",
      "+-----+-----+\n",
      "|   a4| null|\n",
      "|   a4| null|\n",
      "|   a4| null|\n",
      "|   a4| null|\n",
      "|   a4| null|\n",
      "+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can convert to any kind of dtype\n",
    "\n",
    "# however if the dtypes are incompatible, it will return the data as null\n",
    "mpg.select(mpg.model, mpg.model.cast(\"int\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Built-in Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The pyspark avg and mean functions are aliases of eachother\n",
    "from pyspark.sql.functions import concat, sum, avg, min, max, count, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import * can be used to import all functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+-----------------+--------+--------+\n",
      "|(sum(hwy) / count(hwy) AS `average_1`)|        average_2|min(hwy)|max(hwy)|\n",
      "+--------------------------------------+-----------------+--------+--------+\n",
      "|                     23.44017094017094|23.44017094017094|      12|      44|\n",
      "+--------------------------------------+-----------------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# functions can be used to manipulate data as well\n",
    "mpg.select(\n",
    "    sum(mpg.hwy) / count(mpg.hwy).alias(\"average_1\"),\n",
    "    avg(mpg.hwy).alias(\"average_2\"),\n",
    "    min(mpg.hwy),\n",
    "    max(mpg.hwy),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|concat(manufacturer, model)|\n",
      "+---------------------------+\n",
      "|                     audia4|\n",
      "|                     audia4|\n",
      "|                     audia4|\n",
      "|                     audia4|\n",
      "|                     audia4|\n",
      "+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# concat() can be used to combine 2 or more columns into 1\n",
    "mpg.select(concat(mpg.manufacturer, mpg.model)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|concat(cyl,  cylinders)|\n",
      "+-----------------------+\n",
      "|            4 cylinders|\n",
      "|            4 cylinders|\n",
      "|            4 cylinders|\n",
      "|            4 cylinders|\n",
      "|            6 cylinders|\n",
      "+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lit('string') will allow you to add a custom string to the concat \n",
    "mpg.select(concat(mpg.cyl, lit(\" cylinders\"))).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More pyspark Functions for String Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|address                                      |\n",
      "+---------------------------------------------+\n",
      "|600 Navarro St ste 600, San Antonio, TX 78205|\n",
      "|3130 Broadway St, San Antonio, TX 78209      |\n",
      "|303 Pearl Pkwy, San Antonio, TX 78215        |\n",
      "|1255 SW Loop 410, San Antonio, TX 78227      |\n",
      "+---------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"address\": [\n",
    "                \"600 Navarro St ste 600, San Antonio, TX 78205\",\n",
    "                \"3130 Broadway St, San Antonio, TX 78209\",\n",
    "                \"303 Pearl Pkwy, San Antonio, TX 78215\",\n",
    "                \"1255 SW Loop 410, San Antonio, TX 78227\",\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "textdf.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+---------+------------------+\n",
      "|address                                      |street_no|street            |\n",
      "+---------------------------------------------+---------+------------------+\n",
      "|600 Navarro St ste 600, San Antonio, TX 78205|600      |Navarro St ste 600|\n",
      "|3130 Broadway St, San Antonio, TX 78209      |3130     |Broadway St       |\n",
      "|303 Pearl Pkwy, San Antonio, TX 78215        |303      |Pearl Pkwy        |\n",
      "|1255 SW Loop 410, San Antonio, TX 78227      |1255     |SW Loop 410       |\n",
      "+---------------------------------------------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# regexp_extract('column', regex_expression) allows us to use regex to look for specific information to extract\n",
    "textdf.select(\n",
    "    \"address\",\n",
    "    regexp_extract(\"address\", r\"^(\\d+)\", 1).alias(\"street_no\"),\n",
    "    regexp_extract(\"address\", r\"^\\d+\\s([\\w\\s]+?),\", 1).alias(\"street\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+---------------------+\n",
      "|address                                      |city_state_zip       |\n",
      "+---------------------------------------------+---------------------+\n",
      "|600 Navarro St ste 600, San Antonio, TX 78205|San Antonio, TX 78205|\n",
      "|3130 Broadway St, San Antonio, TX 78209      |San Antonio, TX 78209|\n",
      "|303 Pearl Pkwy, San Antonio, TX 78215        |San Antonio, TX 78215|\n",
      "|1255 SW Loop 410, San Antonio, TX 78227      |San Antonio, TX 78227|\n",
      "+---------------------------------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# regexp_replace('column', regex, 'repacement') allows us to make substitutions\n",
    "\n",
    "# in this case everything before the first comma is replaced by an empty space\n",
    "textdf.select(\n",
    "    \"address\",\n",
    "    regexp_replace(\"address\", r\"^.*?,\\s*\", \"\").alias(\"city_state_zip\"),\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# .filter and .where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "|manufacturer|      model|displ|year|cyl|     trans|drv|cty|hwy| fl|     class|\n",
      "+------------+-----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "|       honda|      civic|  1.6|1999|  4|manual(m5)|  f| 28| 33|  r|subcompact|\n",
      "|       honda|      civic|  1.6|1999|  4|  auto(l4)|  f| 24| 32|  r|subcompact|\n",
      "|       honda|      civic|  1.6|1999|  4|manual(m5)|  f| 25| 32|  r|subcompact|\n",
      "|       honda|      civic|  1.6|1999|  4|manual(m5)|  f| 23| 29|  p|subcompact|\n",
      "|       honda|      civic|  1.6|1999|  4|  auto(l4)|  f| 24| 32|  r|subcompact|\n",
      "|       honda|      civic|  1.8|2008|  4|manual(m5)|  f| 26| 34|  r|subcompact|\n",
      "|       honda|      civic|  1.8|2008|  4|  auto(l5)|  f| 25| 36|  r|subcompact|\n",
      "|       honda|      civic|  1.8|2008|  4|  auto(l5)|  f| 24| 36|  c|subcompact|\n",
      "|       honda|      civic|  2.0|2008|  4|manual(m6)|  f| 21| 29|  p|subcompact|\n",
      "|     hyundai|    tiburon|  2.0|1999|  4|  auto(l4)|  f| 19| 26|  r|subcompact|\n",
      "|     hyundai|    tiburon|  2.0|1999|  4|manual(m5)|  f| 19| 29|  r|subcompact|\n",
      "|     hyundai|    tiburon|  2.0|2008|  4|manual(m5)|  f| 20| 28|  r|subcompact|\n",
      "|     hyundai|    tiburon|  2.0|2008|  4|  auto(l4)|  f| 20| 27|  r|subcompact|\n",
      "|      subaru|impreza awd|  2.2|1999|  4|  auto(l4)|  4| 21| 26|  r|subcompact|\n",
      "|      subaru|impreza awd|  2.2|1999|  4|manual(m5)|  4| 19| 26|  r|subcompact|\n",
      "|      subaru|impreza awd|  2.5|1999|  4|manual(m5)|  4| 19| 26|  r|subcompact|\n",
      "|      subaru|impreza awd|  2.5|1999|  4|  auto(l4)|  4| 19| 26|  r|subcompact|\n",
      "|  volkswagen| new beetle|  1.9|1999|  4|manual(m5)|  f| 35| 44|  d|subcompact|\n",
      "|  volkswagen| new beetle|  1.9|1999|  4|  auto(l4)|  f| 29| 41|  d|subcompact|\n",
      "|  volkswagen| new beetle|  2.0|1999|  4|manual(m5)|  f| 21| 29|  r|subcompact|\n",
      "+------------+-----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .filter and .where both return a df where only the specified conditions are met\n",
    "mpg.filter(mpg.cyl == 4).where(mpg[\"class\"] == \"subcompact\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When and Otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "|hwy|    mpg_desc|\n",
      "+---+------------+\n",
      "| 29|good_mileage|\n",
      "| 29|good_mileage|\n",
      "| 31|good_mileage|\n",
      "| 30|good_mileage|\n",
      "| 26|good_mileage|\n",
      "| 26|good_mileage|\n",
      "| 27|good_mileage|\n",
      "| 26|good_mileage|\n",
      "| 25|        null|\n",
      "| 28|good_mileage|\n",
      "| 27|good_mileage|\n",
      "| 25|        null|\n",
      "+---+------------+\n",
      "only showing top 12 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when(column = condtion, 'result') will make a new feature based off of the condition\n",
    "mpg.select(mpg.hwy, when(mpg.hwy > 25, \"good_mileage\").alias(\"mpg_desc\")).show(\n",
    "    12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+\n",
      "|hwy|    mpg_desc|\n",
      "+---+------------+\n",
      "| 29|good_mileage|\n",
      "| 29|good_mileage|\n",
      "| 31|good_mileage|\n",
      "| 30|good_mileage|\n",
      "| 26|good_mileage|\n",
      "| 26|good_mileage|\n",
      "| 27|good_mileage|\n",
      "| 26|good_mileage|\n",
      "| 25| bad_mileage|\n",
      "| 28|good_mileage|\n",
      "| 27|good_mileage|\n",
      "| 25| bad_mileage|\n",
      "+---+------------+\n",
      "only showing top 12 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if the condition is not met in a when statement the observation will return null\n",
    "\n",
    "# otherwise('other_result') can specify a value if the condition is not met\n",
    "mpg.select(\n",
    "    mpg.hwy,\n",
    "    when(mpg.hwy > 25, \"good_mileage\")\n",
    "    .otherwise(\"bad_mileage\")\n",
    "    .alias(\"mpg_desc\"),\n",
    ").show(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "|displ|engine_size|\n",
      "+-----+-----------+\n",
      "|  1.8|      small|\n",
      "|  1.8|      small|\n",
      "|  2.0|     medium|\n",
      "|  2.0|     medium|\n",
      "|  2.8|     medium|\n",
      "|  2.8|     medium|\n",
      "|  3.1|      large|\n",
      "|  1.8|      small|\n",
      "|  1.8|      small|\n",
      "|  2.0|     medium|\n",
      "+-----+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# can do multiple when() statements for multiple values\n",
    "mpg.select(\n",
    "    mpg.displ,\n",
    "    (\n",
    "        when(mpg.displ < 2, \"small\")\n",
    "        .when(mpg.displ < 3, \"medium\")\n",
    "        .otherwise(\"large\")\n",
    "        .alias(\"engine_size\")\n",
    "    ),\n",
    ").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting and Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-----+----+---+----------+---+---+---+---+------+\n",
      "|manufacturer|              model|displ|year|cyl|     trans|drv|cty|hwy| fl| class|\n",
      "+------------+-------------------+-----+----+---+----------+---+---+---+---+------+\n",
      "|       dodge|ram 1500 pickup 4wd|  4.7|2008|  8|manual(m6)|  4|  9| 12|  e|pickup|\n",
      "|       dodge|  dakota pickup 4wd|  4.7|2008|  8|  auto(l5)|  4|  9| 12|  e|pickup|\n",
      "|       dodge|        durango 4wd|  4.7|2008|  8|  auto(l5)|  4|  9| 12|  e|   suv|\n",
      "|        jeep| grand cherokee 4wd|  4.7|2008|  8|  auto(l5)|  4|  9| 12|  e|   suv|\n",
      "|       dodge|ram 1500 pickup 4wd|  4.7|2008|  8|  auto(l5)|  4|  9| 12|  e|pickup|\n",
      "|        jeep| grand cherokee 4wd|  6.1|2008|  8|  auto(l5)|  4| 11| 14|  p|   suv|\n",
      "|   chevrolet|    k1500 tahoe 4wd|  5.3|2008|  8|  auto(l4)|  4| 11| 14|  e|   suv|\n",
      "|  land rover|        range rover|  4.0|1999|  8|  auto(l4)|  4| 11| 15|  p|   suv|\n",
      "+------------+-------------------+-----+----+---+----------+---+---+---+---+------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sort(column) will sort the dataframe by the values in specified column\n",
    "mpg.sort(mpg.hwy).show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default all sort() functions are in ascending order, if you want descending you must import it\n",
    "from pyspark.sql.functions import asc, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "|manufacturer|     model|displ|year|cyl|     trans|drv|cty|hwy| fl|     class|\n",
      "+------------+----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "|  volkswagen|     jetta|  1.9|1999|  4|manual(m5)|  f| 33| 44|  d|   compact|\n",
      "|  volkswagen|new beetle|  1.9|1999|  4|manual(m5)|  f| 35| 44|  d|subcompact|\n",
      "|  volkswagen|new beetle|  1.9|1999|  4|  auto(l4)|  f| 29| 41|  d|subcompact|\n",
      "|      toyota|   corolla|  1.8|2008|  4|manual(m5)|  f| 28| 37|  r|   compact|\n",
      "|       honda|     civic|  1.8|2008|  4|  auto(l5)|  f| 24| 36|  c|subcompact|\n",
      "+------------+----------+-----+----+---+----------+---+---+---+---+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## .sort(column.desc()) changes the sort to descending\n",
    "mpg.sort(mpg.hwy.desc())\n",
    "# is the same as\n",
    "mpg.sort(col(\"hwy\").desc())\n",
    "# is the same as\n",
    "mpg.sort(desc(\"hwy\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+-----+----+---+----------+---+---+---+---+-----+\n",
      "|manufacturer|             model|displ|year|cyl|     trans|drv|cty|hwy| fl|class|\n",
      "+------------+------------------+-----+----+---+----------+---+---+---+---+-----+\n",
      "|      subaru|      forester awd|  2.5|2008|  4|manual(m5)|  4| 20| 27|  r|  suv|\n",
      "|      subaru|      forester awd|  2.5|2008|  4|  auto(l4)|  4| 20| 26|  r|  suv|\n",
      "|      subaru|      forester awd|  2.5|1999|  4|manual(m5)|  4| 18| 25|  r|  suv|\n",
      "|      subaru|      forester awd|  2.5|2008|  4|manual(m5)|  4| 19| 25|  p|  suv|\n",
      "|      subaru|      forester awd|  2.5|1999|  4|  auto(l4)|  4| 18| 24|  r|  suv|\n",
      "|      subaru|      forester awd|  2.5|2008|  4|  auto(l4)|  4| 18| 23|  p|  suv|\n",
      "|      toyota|       4runner 4wd|  2.7|1999|  4|manual(m5)|  4| 15| 20|  r|  suv|\n",
      "|      toyota|       4runner 4wd|  2.7|1999|  4|  auto(l4)|  4| 16| 20|  r|  suv|\n",
      "|        jeep|grand cherokee 4wd|  3.0|2008|  6|  auto(l5)|  4| 17| 22|  d|  suv|\n",
      "|        jeep|grand cherokee 4wd|  4.0|1999|  6|  auto(l4)|  4| 15| 20|  r|  suv|\n",
      "|      toyota|       4runner 4wd|  4.0|2008|  6|  auto(l5)|  4| 16| 20|  r|  suv|\n",
      "|      nissan|    pathfinder 4wd|  4.0|2008|  6|  auto(l5)|  4| 14| 20|  p|  suv|\n",
      "|        ford|      explorer 4wd|  4.0|2008|  6|  auto(l5)|  4| 13| 19|  r|  suv|\n",
      "|      toyota|       4runner 4wd|  3.4|1999|  6|  auto(l4)|  4| 15| 19|  r|  suv|\n",
      "|     mercury|   mountaineer 4wd|  4.0|2008|  6|  auto(l5)|  4| 13| 19|  r|  suv|\n",
      "|        ford|      explorer 4wd|  4.0|1999|  6|manual(m5)|  4| 15| 19|  r|  suv|\n",
      "|        jeep|grand cherokee 4wd|  3.7|2008|  6|  auto(l5)|  4| 15| 19|  r|  suv|\n",
      "|        ford|      explorer 4wd|  4.0|1999|  6|  auto(l5)|  4| 14| 17|  r|  suv|\n",
      "|        ford|      explorer 4wd|  4.0|1999|  6|  auto(l5)|  4| 14| 17|  r|  suv|\n",
      "|      nissan|    pathfinder 4wd|  3.3|1999|  6|  auto(l4)|  4| 14| 17|  r|  suv|\n",
      "+------------+------------------+-----+----+---+----------+---+---+---+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can sort by multiple columns\n",
    "\n",
    "## the column called first will be sorted first, and each column will be sorted in the order they are called\n",
    "mpg.sort(desc(\"class\"), mpg.cyl.asc(), col(\"hwy\").desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping and Aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7f98e97a70a0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .groupby(column) allows you to view the data by making the values of that feature your observations\n",
    "mpg.groupBy(mpg.cyl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+-----------------+\n",
      "|cyl|          avg(cty)|         avg(hwy)|\n",
      "+---+------------------+-----------------+\n",
      "|  6| 16.21518987341772|22.82278481012658|\n",
      "|  5|              20.5|            28.75|\n",
      "|  8|12.571428571428571|17.62857142857143|\n",
      "|  4|21.012345679012345|28.80246913580247|\n",
      "+---+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# groupby requires aggregate functions to work\n",
    "mpg.groupBy(mpg.cyl).agg(avg(mpg.cty), avg(mpg.hwy)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------------+------------------+\n",
      "|cyl|     class|          avg(cty)|          avg(hwy)|\n",
      "+---+----------+------------------+------------------+\n",
      "|  5|   compact|              21.0|              29.0|\n",
      "|  5|subcompact|              20.0|              28.5|\n",
      "|  6|subcompact|              17.0|24.714285714285715|\n",
      "|  6|    pickup|              14.5|              17.9|\n",
      "|  4|subcompact|22.857142857142858| 30.80952380952381|\n",
      "|  8|       suv|12.131578947368421|16.789473684210527|\n",
      "|  8|    pickup|              11.8|              15.8|\n",
      "|  8|   midsize|              16.0|              24.0|\n",
      "|  4|   midsize|              20.5|           29.1875|\n",
      "|  8|   2seater|              15.4|              24.8|\n",
      "|  6|   compact|16.923076923076923|25.307692307692307|\n",
      "|  6|   minivan|              15.6|              22.2|\n",
      "|  4|   compact|            21.375|          29.46875|\n",
      "|  8|subcompact|              14.8|              21.6|\n",
      "|  6|   midsize|17.782608695652176| 26.26086956521739|\n",
      "|  4|   minivan|              18.0|              24.0|\n",
      "|  4|    pickup|              16.0|20.666666666666668|\n",
      "|  6|       suv|              14.5|              18.5|\n",
      "|  4|       suv|              18.0|             23.75|\n",
      "+---+----------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can groupby multiple features if you wish\n",
    "mpg.groupBy(\"cyl\", \"class\").agg(avg(mpg.cty), avg(mpg.hwy)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| cyl|count|\n",
      "+----+-----+\n",
      "|null|  234|\n",
      "|   4|   81|\n",
      "|   5|    4|\n",
      "|   6|   79|\n",
      "|   8|   70|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .rollup() will do the same aggregations as groupby, but will also do total\n",
    "mpg.rollup(\"cyl\").count().sort(\"cyl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "| cyl|         avg(hwy)|\n",
      "+----+-----------------+\n",
      "|null|23.44017094017094|\n",
      "|   4|28.80246913580247|\n",
      "|   5|            28.75|\n",
      "|   6|22.82278481012658|\n",
      "|   8|17.62857142857143|\n",
      "+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.rollup(\"cyl\").agg(expr(\"avg(hwy)\")).sort(\"cyl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------------------+\n",
      "| cyl|     class|          avg(hwy)|\n",
      "+----+----------+------------------+\n",
      "|null|      null| 23.44017094017094|\n",
      "|   4|      null| 28.80246913580247|\n",
      "|   4|   compact|          29.46875|\n",
      "|   4|   midsize|           29.1875|\n",
      "|   4|   minivan|              24.0|\n",
      "|   4|    pickup|20.666666666666668|\n",
      "|   4|subcompact| 30.80952380952381|\n",
      "|   4|       suv|             23.75|\n",
      "|   5|      null|             28.75|\n",
      "|   5|   compact|              29.0|\n",
      "|   5|subcompact|              28.5|\n",
      "|   6|      null| 22.82278481012658|\n",
      "|   6|   compact|25.307692307692307|\n",
      "|   6|   midsize| 26.26086956521739|\n",
      "|   6|   minivan|              22.2|\n",
      "|   6|    pickup|              17.9|\n",
      "|   6|subcompact|24.714285714285715|\n",
      "|   6|       suv|              18.5|\n",
      "|   8|      null| 17.62857142857143|\n",
      "|   8|   2seater|              24.8|\n",
      "+----+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.rollup(\"cyl\", \"class\").mean(\"hwy\").sort(col(\"cyl\"), col(\"class\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crosstabs and Pivot Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+---+---+---+\n",
      "| class_cyl|  4|  5|  6|  8|\n",
      "+----------+---+---+---+---+\n",
      "|   midsize| 16|  0| 23|  2|\n",
      "|subcompact| 21|  2|  7|  5|\n",
      "|   2seater|  0|  0|  0|  5|\n",
      "|    pickup|  3|  0| 10| 20|\n",
      "|   minivan|  1|  0| 10|  0|\n",
      "|       suv|  8|  0| 16| 38|\n",
      "|   compact| 32|  2| 13|  0|\n",
      "+----------+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .crosstab(col1, col2) will make a crosstab table of 2 columns\n",
    "mpg.crosstab(\"class\", \"cyl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----+------------------+------------------+\n",
      "|     class|                 4|   5|                 6|                 8|\n",
      "+----------+------------------+----+------------------+------------------+\n",
      "|subcompact| 30.80952380952381|28.5|24.714285714285715|              21.6|\n",
      "|   compact|          29.46875|29.0|25.307692307692307|              null|\n",
      "|   minivan|              24.0|null|              22.2|              null|\n",
      "|       suv|             23.75|null|              18.5|16.789473684210527|\n",
      "|   midsize|           29.1875|null| 26.26086956521739|              24.0|\n",
      "|    pickup|20.666666666666668|null|              17.9|              15.8|\n",
      "|   2seater|              null|null|              null|              24.8|\n",
      "+----------+------------------+----+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# crosstab simply does counting\n",
    "\n",
    "# pivot(col2).aggregate allows you to change the data to the value of an aggregate functions\n",
    "\n",
    "## in this case the table will be class and cyl, and the data will be the mean hwy value for each combination\n",
    "mpg.groupby(\"class\").pivot(\"cyl\").mean(\"hwy\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|1.0|NaN|\n",
      "|2.0|0.0|\n",
      "|NaN|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "|NaN|NaN|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create df that has missing values\n",
    "df = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\"x\": [1, 2, np.nan, 4, 5, np.nan], \"y\": [np.nan, 0, 0, 3, 1, np.nan]}\n",
    "    )\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|2.0|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .na.drop() will remove all observations with missing data\n",
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|1.0|0.0|\n",
      "|2.0|0.0|\n",
      "|0.0|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "|0.0|0.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .na.fill(value) will replace all missing data with the specificed value\n",
    "df.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|1.0|NaN|\n",
      "|2.0|0.0|\n",
      "|0.0|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "|0.0|NaN|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# subset = 'column' will ensure that only data in the specfied column is replaced if it's null\n",
    "df.na.fill(0, subset=\"x\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  x|  y|\n",
      "+---+---+\n",
      "|2.0|0.0|\n",
      "|NaN|0.0|\n",
      "|4.0|3.0|\n",
      "|5.0|1.0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(subset=\"y\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explaining DataFrame Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[manufacturer#266,model#267,displ#268,year#269L,cyl#270L,trans#271,drv#272,cty#273L,hwy#274L,fl#275,class#276]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# .explain() will demonstrate what spark is thinking about the dataframe\n",
    "mpg.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [cyl#270L, hwy#274L]\n",
      "+- *(1) Scan ExistingRDD[manufacturer#266,model#267,displ#268,year#269L,cyl#270L,trans#271,drv#272,cty#273L,hwy#274L,fl#275,class#276]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.cyl, mpg.hwy).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [(cast((cyl#270L + hwy#274L) as double) / 2.0) AS avg_mpg#1619]\n",
      "+- *(1) Scan ExistingRDD[manufacturer#266,model#267,displ#268,year#269L,cyl#270L,trans#271,drv#272,cty#273L,hwy#274L,fl#275,class#276]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(((mpg.cyl + mpg.hwy) / 2).alias(\"avg_mpg\")).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(cyl#270L) AND (cyl#270L = 6))\n",
      "+- *(1) Scan ExistingRDD[manufacturer#266,model#267,displ#268,year#269L,cyl#270L,trans#271,drv#272,cty#273L,hwy#274L,fl#275,class#276]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.filter(mpg.cyl == 6).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [cyl#270L, hwy#274L]\n",
      "+- *(1) Filter (isnotnull(cyl#270L) AND (cyl#270L = 6))\n",
      "   +- *(1) Scan ExistingRDD[manufacturer#266,model#267,displ#268,year#269L,cyl#270L,trans#271,drv#272,cty#273L,hwy#274L,fl#275,class#276]\n",
      "\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [cyl#270L, hwy#274L]\n",
      "+- *(1) Filter (isnotnull(cyl#270L) AND (cyl#270L = 6))\n",
      "   +- *(1) Scan ExistingRDD[manufacturer#266,model#267,displ#268,year#269L,cyl#270L,trans#271,drv#272,cty#273L,hwy#274L,fl#275,class#276]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(\"cyl\", \"hwy\").filter(expr(\"cyl = 6\")).explain()\n",
    "mpg.filter(expr(\"cyl = 6\")).select(\"cyl\", \"hwy\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [hwy#274L]\n",
      "+- *(1) Scan ExistingRDD[manufacturer#266,model#267,displ#268,year#269L,cyl#270L,trans#271,drv#272,cty#273L,hwy#274L,fl#275,class#276]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.selectExpr(\"cyl + 3 * 16 / 4 + 19 AS unused\", \"hwy\").select(\n",
    "    \"hwy\"\n",
    ").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[], functions=[min(cyl#270L)])\n",
      "+- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#737]\n",
      "   +- *(1) HashAggregate(keys=[], functions=[partial_min(cyl#270L)])\n",
      "      +- *(1) Project [cyl#270L]\n",
      "         +- *(1) Scan ExistingRDD[manufacturer#266,model#267,displ#268,year#269L,cyl#270L,trans#271,drv#272,cty#273L,hwy#274L,fl#275,class#276]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(min(mpg.cyl)).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[cyl#270L], functions=[min(hwy#274L), max(hwy#274L)])\n",
      "+- Exchange hashpartitioning(cyl#270L, 200), ENSURE_REQUIREMENTS, [id=#758]\n",
      "   +- *(1) HashAggregate(keys=[cyl#270L], functions=[partial_min(hwy#274L), partial_max(hwy#274L)])\n",
      "      +- *(1) Project [cyl#270L, hwy#274L]\n",
      "         +- *(1) Scan ExistingRDD[manufacturer#266,model#267,displ#268,year#269L,cyl#270L,trans#271,drv#272,cty#273L,hwy#274L,fl#275,class#276]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.groupby(mpg.cyl).agg(min(mpg.hwy), max(mpg.hwy)).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[cyl#270L], functions=[min(avg_mpg#1656), avg(avg_mpg#1656), max(avg_mpg#1656)])\n",
      "+- Exchange hashpartitioning(cyl#270L, 200), ENSURE_REQUIREMENTS, [id=#783]\n",
      "   +- *(1) HashAggregate(keys=[cyl#270L], functions=[partial_min(avg_mpg#1656), partial_avg(avg_mpg#1656), partial_max(avg_mpg#1656)])\n",
      "      +- *(1) Project [cyl#270L, (cast((cty#273L + hwy#274L) as double) / 2.0) AS avg_mpg#1656]\n",
      "         +- *(1) Filter (isnotnull(class#276) AND (class#276 = compact))\n",
      "            +- *(1) Scan ExistingRDD[manufacturer#266,model#267,displ#268,year#269L,cyl#270L,trans#271,drv#272,cty#273L,hwy#274L,fl#275,class#276]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    mpg.select(col(\"cyl\"), expr(\"(cty + hwy) / 2 AS avg_mpg\"))\n",
    "    .filter(expr('class == \"compact\"'))\n",
    "    .groupby(\"cyl\")\n",
    "    .agg(min(\"avg_mpg\"), avg(\"avg_mpg\"), max(\"avg_mpg\"))\n",
    "    .explain()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Dataframe Manipulation Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------+--------+----+-------+\n",
      "|      date|precipitation|temp_max|temp_min|wind|weather|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "|2012-01-01|          0.0|    12.8|     5.0| 4.7|drizzle|\n",
      "|2012-01-02|         10.9|    10.6|     2.8| 4.5|   rain|\n",
      "|2012-01-03|          0.8|    11.7|     7.2| 2.3|   rain|\n",
      "|2012-01-04|         20.3|    12.2|     5.6| 4.7|   rain|\n",
      "|2012-01-05|          1.3|     8.9|     2.8| 6.1|   rain|\n",
      "|2012-01-06|          2.5|     4.4|     2.2| 2.2|   rain|\n",
      "+----------+-------------+--------+--------+----+-------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vega_datasets import data\n",
    "\n",
    "weather = data.seattle_weather().assign(date=lambda df: df.date.astype(str))\n",
    "weather = spark.createDataFrame(weather)\n",
    "weather.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1461 rows 6 columns\n"
     ]
    }
   ],
   "source": [
    "# unlike pandas there is no .shape command, you have to print a count of rows and columns\n",
    "print(weather.count(), \"rows\", len(weather.columns), \"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2012-01-01', '2015-12-31')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the date range of the first and last date in the dataframe\n",
    "min_date, max_date = weather.select(min(\"date\"), max(\"date\")).first()\n",
    "min_date, max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = weather.withColumn(\n",
    "    \"temp_avg\", expr(\"ROUND(temp_min + temp_max) / 2\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------+--------+----+-------+--------+\n",
      "|      date|precipitation|temp_max|temp_min|wind|weather|temp_avg|\n",
      "+----------+-------------+--------+--------+----+-------+--------+\n",
      "|2012-01-01|          0.0|    12.8|     5.0| 4.7|drizzle|     9.0|\n",
      "|2012-01-02|         10.9|    10.6|     2.8| 4.5|   rain|     6.5|\n",
      "|2012-01-03|          0.8|    11.7|     7.2| 2.3|   rain|     9.5|\n",
      "|2012-01-04|         20.3|    12.2|     5.6| 4.7|   rain|     9.0|\n",
      "|2012-01-05|          1.3|     8.9|     2.8| 6.1|   rain|     6.0|\n",
      "|2012-01-06|          2.5|     4.4|     2.2| 2.2|   rain|     3.5|\n",
      "+----------+-------------+--------+--------+----+-------+--------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month, year, quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|month|    total_rainfall|\n",
      "+-----+------------------+\n",
      "|    1|465.99999999999994|\n",
      "|    2|             422.0|\n",
      "|    3|             606.2|\n",
      "|    4|             375.4|\n",
      "|    5|             207.5|\n",
      "|    6|             132.9|\n",
      "|    7|              48.2|\n",
      "|    8|             163.7|\n",
      "|    9|235.49999999999997|\n",
      "|   10|             503.4|\n",
      "|   11|             642.5|\n",
      "|   12|             622.7|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    weather.withColumn(\"month\", month(\"date\"))\n",
    "    .groupBy(\"month\")\n",
    "    .agg(sum(\"precipitation\").alias(\"total_rainfall\"))\n",
    "    .sort(\"month\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|weather|    avg(temp_avg)|\n",
      "+-------+-----------------+\n",
      "|    fog|7.555555555555555|\n",
      "|    sun|2.977272727272727|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    weather.filter(month(\"date\") == 12)\n",
    "    .filter(year(\"date\") == 2013)\n",
    "    .groupBy(\"weather\")\n",
    "    .agg(mean(\"temp_avg\"))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------------------+\n",
      "|month|no_of_days_with_freezing_temps|\n",
      "+-----+------------------------------+\n",
      "|    1|                             3|\n",
      "|    2|                             0|\n",
      "|    3|                             0|\n",
      "|    4|                             0|\n",
      "|    5|                             0|\n",
      "|    6|                             0|\n",
      "|    7|                             0|\n",
      "|    8|                             0|\n",
      "|    9|                             0|\n",
      "|   10|                             0|\n",
      "|   11|                             0|\n",
      "|   12|                             5|\n",
      "+-----+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    weather.filter(year(\"date\") == 2013)\n",
    "    .withColumn(\"freezing_temps\", (weather.temp_avg <= 0).cast(\"int\"))\n",
    "    .withColumn(\"month\", month(\"date\"))\n",
    "    .groupBy(\"month\")\n",
    "    .agg(sum(\"freezing_temps\").alias(\"no_of_days_with_freezing_temps\"))\n",
    "    .sort(\"month\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------------------+\n",
      "|year|quarter|          temp_avg|\n",
      "+----+-------+------------------+\n",
      "|2012|      1| 5.587912087912088|\n",
      "|2012|      2|12.675824175824175|\n",
      "|2012|      3|            18.375|\n",
      "|2012|      4| 8.581521739130435|\n",
      "|2013|      1| 6.405555555555556|\n",
      "|2013|      2|14.505494505494505|\n",
      "|2013|      3| 19.47826086956522|\n",
      "|2013|      4| 8.032608695652174|\n",
      "|2014|      1| 7.205555555555556|\n",
      "|2014|      2|14.296703296703297|\n",
      "|2014|      3|19.858695652173914|\n",
      "|2014|      4|  9.88586956521739|\n",
      "|2015|      1| 8.972222222222221|\n",
      "|2015|      2|15.258241758241759|\n",
      "|2015|      3|19.407608695652176|\n",
      "|2015|      4| 8.956521739130435|\n",
      "+----+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    weather.withColumn(\"quarter\", quarter(\"date\"))\n",
    "    .withColumn(\"year\", year(\"date\"))\n",
    "    .groupBy(\"year\", \"quarter\")\n",
    "    .agg(mean(\"temp_avg\").alias(\"temp_avg\"))\n",
    "    .sort(\"year\", \"quarter\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+-----+-----+\n",
      "|quarter| 2012| 2013| 2014| 2015|\n",
      "+-------+-----+-----+-----+-----+\n",
      "|      1| 5.59| 6.41| 7.21| 8.97|\n",
      "|      2|12.68|14.51| 14.3|15.26|\n",
      "|      3|18.38|19.48|19.86|19.41|\n",
      "|      4| 8.58| 8.03| 9.89| 8.96|\n",
      "+-------+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    weather.withColumn(\"quarter\", quarter(\"date\"))\n",
    "    .withColumn(\"year\", year(\"date\"))\n",
    "    .groupBy(\"quarter\")\n",
    "    .pivot(\"year\")\n",
    "    .agg(expr(\"ROUND(MEAN(temp_avg), 2) AS temp_avg\"))\n",
    "    .sort(\"quarter\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- users ---\n",
      "+---+-----+-------+\n",
      "| id| name|role_id|\n",
      "+---+-----+-------+\n",
      "|  1|  bob|    1.0|\n",
      "|  2|  joe|    2.0|\n",
      "|  3|sally|    3.0|\n",
      "|  4| adam|    3.0|\n",
      "|  5| jane|    NaN|\n",
      "|  6| mike|    NaN|\n",
      "+---+-----+-------+\n",
      "\n",
      "--- roles ---\n",
      "+---+---------+\n",
      "| id|     name|\n",
      "+---+---------+\n",
      "|  1|    admin|\n",
      "|  2|   author|\n",
      "|  3| reviewer|\n",
      "|  4|commenter|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"id\": [1, 2, 3, 4, 5, 6],\n",
    "            \"name\": [\"bob\", \"joe\", \"sally\", \"adam\", \"jane\", \"mike\"],\n",
    "            \"role_id\": [1, 2, 3, 3, np.nan, np.nan],\n",
    "        }\n",
    "    )\n",
    ")\n",
    "roles = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"id\": [1, 2, 3, 4],\n",
    "            \"name\": [\"admin\", \"author\", \"reviewer\", \"commenter\"],\n",
    "        }\n",
    "    )\n",
    ")\n",
    "print(\"--- users ---\")\n",
    "users.show()\n",
    "print(\"--- roles ---\")\n",
    "roles.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+---+--------+\n",
      "| id| name|role_id| id|    name|\n",
      "+---+-----+-------+---+--------+\n",
      "|  1|  bob|    1.0|  1|   admin|\n",
      "|  3|sally|    3.0|  3|reviewer|\n",
      "|  4| adam|    3.0|  3|reviewer|\n",
      "|  2|  joe|    2.0|  2|  author|\n",
      "+---+-----+-------+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# join tables by specifying which column on each table is the same\n",
    "users.join(roles, on=users.role_id == roles.id).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+----+--------+\n",
      "| id| name|role_id|  id|    name|\n",
      "+---+-----+-------+----+--------+\n",
      "|  5| jane|    NaN|null|    null|\n",
      "|  6| mike|    NaN|null|    null|\n",
      "|  1|  bob|    1.0|   1|   admin|\n",
      "|  3|sally|    3.0|   3|reviewer|\n",
      "|  4| adam|    3.0|   3|reviewer|\n",
      "|  2|  joe|    2.0|   2|  author|\n",
      "+---+-----+-------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# by default spark will perform an inner join unless specified otherwise\n",
    "users.join(roles, on=users.role_id == roles.id, how=\"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-------+---+---------+\n",
      "|  id| name|role_id| id|     name|\n",
      "+----+-----+-------+---+---------+\n",
      "|   1|  bob|    1.0|  1|    admin|\n",
      "|null| null|   null|  4|commenter|\n",
      "|   3|sally|    3.0|  3| reviewer|\n",
      "|   4| adam|    3.0|  3| reviewer|\n",
      "|   2|  joe|    2.0|  2|   author|\n",
      "+----+-----+-------+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users.join(roles, on=users.role_id == roles.id, how=\"right\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization (or Lack Therof)\n",
    "\n",
    "- there is no visualizations for spark\n",
    "\n",
    "- if you wish to make visualizations you must convert the dataframe to pandas using .toPandas and then make pandas visualizations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a spark data frame that contains your favorite programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = {'languages': ['python', 'java', 'html', 'javascript', 'typescript', 'c#', 'c++', 'c', 'sql']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The name of the column should be language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pd.DataFrame(languages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| languages|\n",
      "+----------+\n",
      "|    python|\n",
      "|      java|\n",
      "|      html|\n",
      "|javascript|\n",
      "|typescript|\n",
      "|        c#|\n",
      "|       c++|\n",
      "|         c|\n",
      "|       sql|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- View the schema of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(languages,StringType,true)))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Output the shape of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 1)\n"
     ]
    }
   ],
   "source": [
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Show the first 5 records in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| languages|\n",
      "+----------+\n",
      "|    python|\n",
      "|      java|\n",
      "|      html|\n",
      "|javascript|\n",
      "|typescript|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the mpg dataset as a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|   a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|   a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|  auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|   a4|  2.8|1999|  6|  auto(l5)|  f| 16| 26|  p|compact|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A. Create 1 column of output that contains a message like the one below:\n",
    "    \n",
    "    'The 1999 audi a4 has a 4 cylinder engine.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+\n",
      "|message                                   |\n",
      "+------------------------------------------+\n",
      "|The 1999 audi has a 4 cylinder engine     |\n",
      "|The 1999 audi has a 4 cylinder engine     |\n",
      "|The 2008 audi has a 4 cylinder engine     |\n",
      "|The 2008 audi has a 4 cylinder engine     |\n",
      "|The 1999 audi has a 6 cylinder engine     |\n",
      "|The 1999 audi has a 6 cylinder engine     |\n",
      "|The 2008 audi has a 6 cylinder engine     |\n",
      "|The 1999 audi has a 4 cylinder engine     |\n",
      "|The 1999 audi has a 4 cylinder engine     |\n",
      "|The 2008 audi has a 4 cylinder engine     |\n",
      "|The 2008 audi has a 4 cylinder engine     |\n",
      "|The 1999 audi has a 6 cylinder engine     |\n",
      "|The 1999 audi has a 6 cylinder engine     |\n",
      "|The 2008 audi has a 6 cylinder engine     |\n",
      "|The 2008 audi has a 6 cylinder engine     |\n",
      "|The 1999 audi has a 6 cylinder engine     |\n",
      "|The 2008 audi has a 6 cylinder engine     |\n",
      "|The 2008 audi has a 8 cylinder engine     |\n",
      "|The 2008 chevrolet has a 8 cylinder engine|\n",
      "|The 2008 chevrolet has a 8 cylinder engine|\n",
      "+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(concat(lit('The '), mpg.year, lit(' '), mpg.manufacturer, lit(' has a '), mpg.cyl, lit(' cylinder engine')).alias('message')).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- B. Transform the trans column so that it only contains either manual or auto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|   a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|   a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|  auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|   a4|  2.8|1999|  6|  auto(l5)|  f| 16| 26|  p|compact|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|     trans| trans|\n",
      "+----------+------+\n",
      "|  auto(l5)|  auto|\n",
      "|manual(m5)|manual|\n",
      "|manual(m6)|manual|\n",
      "|  auto(av)|  auto|\n",
      "|  auto(l5)|  auto|\n",
      "|manual(m5)|manual|\n",
      "|  auto(av)|  auto|\n",
      "|manual(m5)|manual|\n",
      "|  auto(l5)|  auto|\n",
      "|manual(m6)|manual|\n",
      "|  auto(s6)|  auto|\n",
      "|  auto(l5)|  auto|\n",
      "|manual(m5)|manual|\n",
      "|  auto(s6)|  auto|\n",
      "|manual(m6)|manual|\n",
      "|  auto(l5)|  auto|\n",
      "|  auto(s6)|  auto|\n",
      "|  auto(s6)|  auto|\n",
      "|  auto(l4)|  auto|\n",
      "|  auto(l4)|  auto|\n",
      "+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mpg.select(mpg.trans,\n",
    "          when((mpg.trans.contains('auto')), 'auto')\n",
    "          .otherwise('manual')\n",
    "          .alias('trans')).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the tips dataset as a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydataset import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips = spark.createDataFrame(data(\"tips\"))\n",
    "\n",
    "tips.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A. What percentage of observations are smokers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|smoker|count|\n",
      "+------+-----+\n",
      "|    No|  151|\n",
      "|   Yes|   93|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips.groupby(tips.smoker).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- B. Create a column that contains the tip percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips2 = tips.select(tips.smoker, tips.sex, tips.total_bill, tips.tip, (tips.tip / tips.total_bill).alias('tip_percent'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+----------+----+-------------------+\n",
      "|smoker|   sex|total_bill| tip|        tip_percent|\n",
      "+------+------+----------+----+-------------------+\n",
      "|    No|Female|     16.99|1.01|0.05944673337257211|\n",
      "|    No|  Male|     10.34|1.66|0.16054158607350097|\n",
      "|    No|  Male|     21.01| 3.5|0.16658733936220846|\n",
      "|    No|  Male|     23.68|3.31| 0.1397804054054054|\n",
      "|    No|Female|     24.59|3.61|0.14680764538430255|\n",
      "+------+------+----------+----+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- C. Calculate the average tip percentage for each combination of sex and smoker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, sum, avg, min, max, count, mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+-------------------+\n",
      "|   sex|                No|                Yes|\n",
      "+------+------------------+-------------------+\n",
      "|Female|0.1569209707691836|0.18215035269941035|\n",
      "|  Male|0.1606687151291298| 0.1527711752024851|\n",
      "+------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tips2.groupby('sex').pivot('smoker').mean('tip_percent').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use the seattle weather dataset referenced in the lesson to answer the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------+--------+----+-------+--------+\n",
      "|      date|precipitation|temp_max|temp_min|wind|weather|temp_avg|\n",
      "+----------+-------------+--------+--------+----+-------+--------+\n",
      "|2012-01-01|          0.0|    12.8|     5.0| 4.7|drizzle|     9.0|\n",
      "|2012-01-02|         10.9|    10.6|     2.8| 4.5|   rain|     6.5|\n",
      "|2012-01-03|          0.8|    11.7|     7.2| 2.3|   rain|     9.5|\n",
      "|2012-01-04|         20.3|    12.2|     5.6| 4.7|   rain|     9.0|\n",
      "|2012-01-05|          1.3|     8.9|     2.8| 6.1|   rain|     6.0|\n",
      "+----------+-------------+--------+--------+----+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert the temperatures to farenheight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|temp_f|\n",
      "+------+\n",
      "|  48.2|\n",
      "|  43.7|\n",
      "|  49.1|\n",
      "|  48.2|\n",
      "|  42.8|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather.select(((weather.temp_avg * (9/5)) + 32).alias('temp_f')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which month has the most rain, on average?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import asc, desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|month|     avg_rainfall|\n",
      "+-----+-----------------+\n",
      "|   11|5.354166666666667|\n",
      "+-----+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    weather.withColumn(\"month\", month(\"date\"))\n",
    "    .groupBy(\"month\")\n",
    "    .agg(mean(\"precipitation\").alias(\"avg_rainfall\"))\n",
    "    .sort(desc(\"avg_rainfall\"))\n",
    "    .show(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which year was the windiest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------+\n",
      "|year|         avg_wind|\n",
      "+----+-----------------+\n",
      "|2012|3.400819672131147|\n",
      "+----+-----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    weather.withColumn(\"year\", year(\"date\"))\n",
    "    .groupBy(\"year\")\n",
    "    .agg(mean(\"wind\").alias(\"avg_wind\"))\n",
    "    .sort(desc(\"avg_wind\"))\n",
    "    .show(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the most frequent type of weather in January?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|weather|count|\n",
      "+-------+-----+\n",
      "|    fog|   38|\n",
      "+-------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    weather.filter(month(\"date\") == 1).\n",
    "    groupby('weather').count().sort(desc('count'))\n",
    "    .show(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What is the average high and low temperature on sunny days in July in 2013 and 2014?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+\n",
      "|     avg(temp_max)|    avg(temp_min)|\n",
      "+------------------+-----------------+\n",
      "|26.828846153846158|14.18269230769231|\n",
      "+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    weather.filter(month(\"date\") == 7)\n",
    "    .filter((year(\"date\") == 2013) | (year(\"date\") == 2014))\n",
    "    .filter(weather.weather == 'sun')\n",
    "    .agg(mean(\"temp_max\"), mean(\"temp_min\"))\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What percentage of days were rainy in q3 of 2015?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           avg(rain)|\n",
      "+--------------------+\n",
      "|0.021739130434782608|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    \n",
    "    weather.filter(quarter('date') == 3).filter(year('date') == 2015)\n",
    "    .select(when(col(\"weather\") == \"rain\", 1).otherwise(0).alias(\"rain\"))\n",
    "    .agg(mean(\"rain\")).show()\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each year, find what percentage of days it rained (had non-zero precipitation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|year(date)|count|\n",
      "+----------+-----+\n",
      "|      2015|  144|\n",
      "|      2013|  152|\n",
      "|      2014|  150|\n",
      "|      2012|  177|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weather.where(weather.precipitation > 0).groupby(year('date')).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
