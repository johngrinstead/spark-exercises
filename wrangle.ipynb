{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- enivronment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark df by reading pdf\n",
    "df = spark.read.csv(\"source.csv\", sep=\",\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------+\n",
      "|source_id| source_username|\n",
      "+---------+----------------+\n",
      "|   100137|Merlene Blodgett|\n",
      "|   103582|     Carmen Cura|\n",
      "|   106463| Richard Sanchez|\n",
      "|   119403|  Betty De Hoyos|\n",
      "|   119555|  Socorro Quiara|\n",
      "+---------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[source_id: string, source_username: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is alsow valid\n",
    "(\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"sep\", \",\")\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"header\", True)\n",
    "    .load(\"source.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions\n",
    "from pyspark.sql.types import StructType, StructField, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[source_id: string, source_username: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make schema object\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"source_id\", StringType()),\n",
    "        StructField(\"source_username\", StringType()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# apply schema to spark df\n",
    "spark.read.csv(\"source.csv\", header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for demo purposes\n",
    "from pydataset import data\n",
    "\n",
    "mpg = spark.createDataFrame(data(\"mpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the data into a file\n",
    "mpg.write.json(\"mpg_json\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# like much else in spark, there's multiple ways we could do this:\n",
    "(\n",
    "    mpg.write.format(\"csv\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .save(\"data/mpg_csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " case_id              | 1014127332           \n",
      " case_opened_date     | 1/1/18 0:42          \n",
      " case_closed_date     | 1/1/18 12:29         \n",
      " SLA_due_date         | 9/26/20 0:42         \n",
      " case_late            | NO                   \n",
      " num_days_late        | -998.5087616000001   \n",
      " case_closed          | YES                  \n",
      " dept_division        | Field Operations     \n",
      " service_request_type | Stray Animal         \n",
      " SLA_days             | 999.0                \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMLS             \n",
      " request_address      | 2315  EL PASO ST,... \n",
      " council_district     | 5                    \n",
      "-RECORD 1------------------------------------\n",
      " case_id              | 1014127333           \n",
      " case_opened_date     | 1/1/18 0:46          \n",
      " case_closed_date     | 1/3/18 8:11          \n",
      " SLA_due_date         | 1/5/18 8:30          \n",
      " case_late            | NO                   \n",
      " num_days_late        | -2.0126041669999997  \n",
      " case_closed          | YES                  \n",
      " dept_division        | Storm Water          \n",
      " service_request_type | Removal Of Obstru... \n",
      " SLA_days             | 4.322222222          \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMSS             \n",
      " request_address      | 2215  GOLIAD RD, ... \n",
      " council_district     | 3                    \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make spark df\n",
    "df = spark.read.csv(\"case.csv\", header=True, inferSchema=True)\n",
    "df.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .withColumnRenamed() to rename columns\n",
    "df = df.withColumnRenamed(\"SLA_due_date\", \"case_due_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correct Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+------+\n",
      "|case_closed|case_late| count|\n",
      "+-----------+---------+------+\n",
      "|         NO|      YES|  6525|\n",
      "|        YES|      YES| 87978|\n",
      "|         NO|       NO| 11585|\n",
      "|        YES|       NO|735616|\n",
      "+-----------+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# demonstrating we only have yes/no in each field\n",
    "df.groupBy(\"case_closed\", \"case_late\").count().show()\n",
    "\n",
    "## these variables are binary, instead of strings they should be booleans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|case_closed|case_late|\n",
      "+-----------+---------+\n",
      "|       true|    false|\n",
      "|       true|    false|\n",
      "|       true|    false|\n",
      "|       true|    false|\n",
      "|       true|     true|\n",
      "+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert dtypes of desired columns\n",
    "df = df.withColumn(\"case_closed\", expr('case_closed == \"YES\"')).withColumn(\n",
    "    \"case_late\", expr('case_late == \"YES\"')\n",
    ")\n",
    "\n",
    "df.select(\"case_closed\", \"case_late\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+\n",
      "|council_district| count|\n",
      "+----------------+------+\n",
      "|               1|119309|\n",
      "|               6| 74095|\n",
      "|               3|102706|\n",
      "|               5|114609|\n",
      "|               9| 40916|\n",
      "|               4| 93778|\n",
      "|               8| 42345|\n",
      "|               7| 72445|\n",
      "|              10| 62926|\n",
      "|               2|114745|\n",
      "|               0|  3830|\n",
      "+----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"council_district\").count().show()\n",
    "# this variable is listed as an integer, but it should be a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the desired column into a string dtype\n",
    "df = df.withColumn(\"council_district\", col(\"council_district\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Before handling dates\n",
      "+-------------------+-------------------+-------------------+\n",
      "|   case_opened_date|   case_closed_date|      case_due_date|\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2018-01-01 00:42:00|2018-01-01 00:42:00|2018-01-01 00:42:00|\n",
      "|2018-01-01 00:46:00|2018-01-01 00:46:00|2018-01-01 00:46:00|\n",
      "|2018-01-01 00:48:00|2018-01-01 00:48:00|2018-01-01 00:48:00|\n",
      "|2018-01-01 01:29:00|2018-01-01 01:29:00|2018-01-01 01:29:00|\n",
      "|2018-01-01 01:34:00|2018-01-01 01:34:00|2018-01-01 01:34:00|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dates are in an incorrect format for machine learning\n",
    "print(\"--- Before handling dates\")\n",
    "df.select(\"case_opened_date\", \"case_closed_date\", \"case_due_date\").show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct to desired format\n",
    "fmt = \"M/d/yy H:mm\"\n",
    "df = (\n",
    "    df.withColumn(\"case_opened_date\", to_timestamp(\"case_opened_date\", fmt))\n",
    "    .withColumn(\"case_closed_date\", to_timestamp(\"case_opened_date\", fmt))\n",
    "    .withColumn(\"case_due_date\", to_timestamp(\"case_opened_date\", fmt))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- After\n",
      "+-------------------+-------------------+-------------------+\n",
      "|   case_opened_date|   case_closed_date|      case_due_date|\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2018-01-01 00:42:00|2018-01-01 00:42:00|2018-01-01 00:42:00|\n",
      "|2018-01-01 00:46:00|2018-01-01 00:46:00|2018-01-01 00:46:00|\n",
      "|2018-01-01 00:48:00|2018-01-01 00:48:00|2018-01-01 00:48:00|\n",
      "|2018-01-01 01:29:00|2018-01-01 01:29:00|2018-01-01 01:29:00|\n",
      "|2018-01-01 01:34:00|2018-01-01 01:34:00|2018-01-01 01:34:00|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- After\")\n",
    "df.select(\"case_opened_date\", \"case_closed_date\", \"case_due_date\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Before\n",
      "+--------------------+\n",
      "|     request_address|\n",
      "+--------------------+\n",
      "|2315  EL PASO ST,...|\n",
      "|2215  GOLIAD RD, ...|\n",
      "|102  PALFREY ST W...|\n",
      "|114  LA GARDE ST,...|\n",
      "|734  CLEARVIEW DR...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "--- After\n",
      "+--------------------+\n",
      "|     request_address|\n",
      "+--------------------+\n",
      "|2315  el paso st,...|\n",
      "|2215  goliad rd, ...|\n",
      "|102  palfrey st w...|\n",
      "|114  la garde st,...|\n",
      "|734  clearview dr...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# normalize the text in the specified column\n",
    "print(\"--- Before\")\n",
    "df.select(\"request_address\").show(5)\n",
    "\n",
    "df = df.withColumn(\"request_address\", trim(lower(df.request_address)))\n",
    "\n",
    "print(\"--- After\")\n",
    "df.select(\"request_address\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|      num_days_late|      num_weeks_late|\n",
      "+-------------------+--------------------+\n",
      "| -998.5087616000001|        -142.6441088|\n",
      "|-2.0126041669999997|-0.28751488099999994|\n",
      "|       -3.022337963|-0.43176256614285713|\n",
      "|       -15.01148148| -2.1444973542857144|\n",
      "|0.37216435200000003|         0.053166336|\n",
      "+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert count of days into count of weeks\n",
    "df = df.withColumn(\n",
    "    \"num_weeks_late\", expr(\"num_days_late / 7 AS num_weeks_late\")\n",
    ")\n",
    "\n",
    "df.select(\"num_days_late\", \"num_weeks_late\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|council_district|\n",
      "+----------------+\n",
      "|             005|\n",
      "|             003|\n",
      "|             003|\n",
      "|             003|\n",
      "|             007|\n",
      "+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# format desired strings using regex\n",
    "df = df.withColumn(\"council_district\", col(\"council_district\").cast(\"int\"))\n",
    "\n",
    "# '%03d' means at least 3 digits, pad with 0s\n",
    "#\n",
    "# In order to use the format_string function the way we are, we'll need to\n",
    "# convert council_district back to an integer temporarily, but the final output\n",
    "# will be a string.\n",
    "df = df.withColumn(\n",
    "    \"council_district\",\n",
    "    format_string(\"%03d\", col(\"council_district\").cast(\"int\")),\n",
    ")\n",
    "\n",
    "df.select(\"council_district\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|zipcode|\n",
      "+-------+\n",
      "|  78207|\n",
      "|  78223|\n",
      "|  78223|\n",
      "|  78223|\n",
      "|  78228|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use regex to extract zipcode, make into it's own feature\n",
    "df = df.withColumn(\"zipcode\", regexp_extract(\"request_address\", r\"\\d+$\", 0))\n",
    "\n",
    "df.select(\"zipcode\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+-------------------+--------+--------------+-------------+\n",
      "|case_closed|   case_opened_date|   case_closed_date|case_age|days_to_closed|case_lifetime|\n",
      "+-----------+-------------------+-------------------+--------+--------------+-------------+\n",
      "|       true|2018-01-01 00:42:00|2018-01-01 00:42:00|    1232|             0|            0|\n",
      "|       true|2018-01-01 00:46:00|2018-01-01 00:46:00|    1232|             0|            0|\n",
      "|       true|2018-01-01 00:48:00|2018-01-01 00:48:00|    1232|             0|            0|\n",
      "|       true|2018-01-01 01:29:00|2018-01-01 01:29:00|    1232|             0|            0|\n",
      "|       true|2018-01-01 01:34:00|2018-01-01 01:34:00|    1232|             0|            0|\n",
      "+-----------+-------------------+-------------------+--------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------+-------------------+-------------------+--------+--------------+-------------+\n",
      "|case_closed|   case_opened_date|   case_closed_date|case_age|days_to_closed|case_lifetime|\n",
      "+-----------+-------------------+-------------------+--------+--------------+-------------+\n",
      "|      false|2018-01-02 09:39:00|2018-01-02 09:39:00|    1231|             0|         1231|\n",
      "|      false|2018-01-02 10:49:00|2018-01-02 10:49:00|    1231|             0|         1231|\n",
      "|      false|2018-01-02 13:45:00|2018-01-02 13:45:00|    1231|             0|         1231|\n",
      "|      false|2018-01-02 14:09:00|2018-01-02 14:09:00|    1231|             0|         1231|\n",
      "|      false|2018-01-02 14:34:00|2018-01-02 14:34:00|    1231|             0|         1231|\n",
      "+-----------+-------------------+-------------------+--------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make new features\n",
    "df = (\n",
    "    df.withColumn(\n",
    "        \"case_age\", datediff(current_timestamp(), \"case_opened_date\")\n",
    "    ) # days between when the case was opened and today\n",
    "    .withColumn(\n",
    "        \"days_to_closed\", datediff(\"case_closed_date\", \"case_opened_date\")\n",
    "    ) # number of days between when the case was opened and the case was closed\n",
    "    .withColumn(\n",
    "        \"case_lifetime\",\n",
    "        when(expr(\"! case_closed\"), col(\"case_age\")).otherwise(\n",
    "            col(\"days_to_closed\")\n",
    "        ), # if the case is still open, how many days has it been open\n",
    "    )\n",
    ")\n",
    "\n",
    "df.select(\n",
    "    \"case_closed\",\n",
    "    \"case_opened_date\",\n",
    "    \"case_closed_date\",\n",
    "    \"case_age\",\n",
    "    \"days_to_closed\",\n",
    "    \"case_lifetime\",\n",
    ").where(expr(\"case_closed\")).show(5)\n",
    "\n",
    "df.select(\n",
    "    \"case_closed\",\n",
    "    \"case_opened_date\",\n",
    "    \"case_closed_date\",\n",
    "    \"case_age\",\n",
    "    \"days_to_closed\",\n",
    "    \"case_lifetime\",\n",
    ").where(expr(\"! case_closed\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining Department Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "|       dept_division|           dept_name|standardized_dept_name|dept_subject_to_SLA|\n",
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "|     311 Call Center|    Customer Service|      Customer Service|                YES|\n",
      "|               Brush|Solid Waste Manag...|           Solid Waste|                YES|\n",
      "|     Clean and Green|Parks and Recreation|    Parks & Recreation|                YES|\n",
      "|Clean and Green N...|Parks and Recreation|    Parks & Recreation|                YES|\n",
      "|    Code Enforcement|Code Enforcement ...|  DSD/Code Enforcement|                YES|\n",
      "+--------------------+--------------------+----------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read other csv into spark df\n",
    "dept = spark.read.csv(\"dept.csv\", header=True, inferSchema=True)\n",
    "dept.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------------------------\n",
      " case_id              | 1014127332           \n",
      " case_opened_date     | 2018-01-01 00:42:00  \n",
      " case_closed_date     | 2018-01-01 00:42:00  \n",
      " case_due_date        | 2018-01-01 00:42:00  \n",
      " case_late            | false                \n",
      " num_days_late        | -998.5087616000001   \n",
      " case_closed          | true                 \n",
      " service_request_type | Stray Animal         \n",
      " SLA_days             | 999.0                \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMLS             \n",
      " request_address      | 2315  el paso st,... \n",
      " council_district     | 005                  \n",
      " num_weeks_late       | -142.6441088         \n",
      " zipcode              | 78207                \n",
      " case_age             | 1232                 \n",
      " days_to_closed       | 0                    \n",
      " case_lifetime        | 0                    \n",
      " department           | Animal Care Services \n",
      " dept_subject_to_SLA  | true                 \n",
      "-RECORD 1------------------------------------\n",
      " case_id              | 1014127333           \n",
      " case_opened_date     | 2018-01-01 00:46:00  \n",
      " case_closed_date     | 2018-01-01 00:46:00  \n",
      " case_due_date        | 2018-01-01 00:46:00  \n",
      " case_late            | false                \n",
      " num_days_late        | -2.0126041669999997  \n",
      " case_closed          | true                 \n",
      " service_request_type | Removal Of Obstru... \n",
      " SLA_days             | 4.322222222          \n",
      " case_status          | Closed               \n",
      " source_id            | svcCRMSS             \n",
      " request_address      | 2215  goliad rd, ... \n",
      " council_district     | 003                  \n",
      " num_weeks_late       | -0.28751488099999994 \n",
      " zipcode              | 78223                \n",
      " case_age             | 1232                 \n",
      " days_to_closed       | 0                    \n",
      " case_lifetime        | 0                    \n",
      " department           | Trans & Cap Impro... \n",
      " dept_subject_to_SLA  | true                 \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (\n",
    "    df\n",
    "    # left join on dept_division\n",
    "    .join(dept, \"dept_division\", \"left\")\n",
    "    # drop all the columns except for standardized name, as it has much fewer unique values\n",
    "    .drop(dept.dept_division)\n",
    "    .drop(dept.dept_name)\n",
    "    .drop(df.dept_division)\n",
    "    .withColumnRenamed(\"standardized_dept_name\", \"department\")\n",
    "    # convert to a boolean\n",
    "    .withColumn(\"dept_subject_to_SLA\", col(\"dept_subject_to_SLA\") == \"YES\")\n",
    ")\n",
    "\n",
    "df.show(2, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very similar to pandas splitting data\n",
    "train, validate, test = df.randomSplit([0.6, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the case, department, and source data into their own spark dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
